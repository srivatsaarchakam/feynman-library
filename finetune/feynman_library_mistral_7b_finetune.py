# -*- coding: utf-8 -*-
"""feynman library mistral 7b finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NraW-qGlB198Te4rFZxfb5TyjgS7RoJP
"""

# !pip install -Uqqq pip
# !pip install -qqq torch==2.3.0
# !pip install -qqq -U git+https://github.com/TimDettmers/bitsandbytes.git@main
# !pip install -qqq -U git+https://github.com/huggingface/transformers.git@main
# !pip install -qqq -U git+https://github.com/huggingface/peft.git@main
# !pip install -qqq -U git+https://github.com/huggingface/accelerate.git@main
# !pip install -qqq datasets==2.12.0
# !pip install -qqq loralib==0.1.1
# !pip install -qqq einops==0.6.1

import json
import os
from pprint import pprint
import torch
import torch.nn as nn
import transformers
from datasets import load_dataset
from huggingface_hub import notebook_login


import bitsandbytes as bnb
from peft import (
    LoraConfig,
    PeftConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training
)

from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

notebook_login()

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")